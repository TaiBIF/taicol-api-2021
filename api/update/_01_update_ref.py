import pandas as pd
import requests
from datetime import datetime
import json
from numpy import nan
import numpy as np
from api.update.utils import DatabaseManager, to_firstname_abbr, to_middlename_abbr, setup_logging
import logging
import time

class CitationUpdater(DatabaseManager):
    """å¼•ç”¨æ›´æ–°å™¨"""
    
    def __init__(self, batch_size=1000, max_retries=3):
        """
        åˆå§‹åŒ– CitationUpdater
        
        Args:
            batch_size (int): æ‰¹æ¬¡æ’å…¥çš„å¤§å°ï¼Œé»˜èªç‚º 1000
            max_retries (int): æœ€å¤§é‡è©¦æ¬¡æ•¸
        """
        super().__init__(max_retries)
        self.batch_size = batch_size
        setup_logging()
    
    def get_last_update_times(self):
        """ç²å–æœ€å¾Œæ›´æ–°æ™‚é–“"""
        queries = {
            'api_citations': "SELECT MAX(updated_at) FROM `api_citations`",
            'references': "SELECT MAX(updated_at) FROM `references` WHERE is_publish = 1"
        }
        
        results = {}
        for key, query in queries.items():
            result = self.execute_with_retry(query, fetch=True)
            results[key] = result[0][0] if result and result[0][0] else datetime(1900, 1, 1)
        
        return results['api_citations'], results['references']
    
    def fetch_updated_references(self, last_updated=None, limit=None, update_all=False, reference_ids=None, person_ids=None, min_reference_id=None):
        """
        ç²å–éœ€è¦æ›´æ–°çš„æ–‡ç»è³‡æ–™ï¼Œç›´æ¥æå– properties ä¸­çš„ç‰¹å®šæ¬„ä½
        
        Args:
            last_updated: æœ€å¾Œæ›´æ–°æ™‚é–“
            limit: é™åˆ¶è¿”å›è¨˜éŒ„æ•¸
            update_all: æ˜¯å¦æ›´æ–°å…¨éƒ¨
            reference_ids: æŒ‡å®šçš„ reference_id åˆ—è¡¨
            person_ids: æŒ‡å®šçš„ person_id åˆ—è¡¨
            min_reference_id: æœ€å° reference_id æ¢ä»¶
        """
        base_query = """
            SELECT p.last_name, p.first_name, p.middle_name, r.id as reference_id, pr.order, 
                   r.publish_year, r.type, r.title, 
                   r.properties ->> '$.doi' as doi,
                   r.properties ->> '$.article_title' as article_title,
                   r.properties ->> '$.book_title' as book_title,
                   r.properties ->> '$.volume' as volume,
                   r.properties ->> '$.issue' as issue,
                   r.properties ->> '$.article_number' as article_number,
                   r.properties ->> '$.pages_range' as pages_range,
                   r.properties ->> '$.edition' as edition,
                   r.properties ->> '$.chapter' as chapter,
                   ac.publish_date             
            FROM `references` r              
            LEFT JOIN person_reference pr ON r.id = pr.reference_id             
            LEFT JOIN persons p ON pr.person_id = p.id             
            LEFT JOIN api_citations ac ON ac.reference_id = r.id             
            WHERE r.is_publish = 1
        """

        # æ ¹æ“šä¸åŒçš„æ›´æ–°æ–¹å¼æ·»åŠ æ¢ä»¶
        params = []
        
        if person_ids:
            # ä½¿ç”¨æŒ‡å®šçš„ person_id åˆ—è¡¨
            if not isinstance(person_ids, (list, tuple)):
                person_ids = [person_ids]
            
            # éæ¿¾ç„¡æ•ˆçš„ ID
            valid_ids = [pid for pid in person_ids if pid is not None and str(pid).strip()]
            
            if not valid_ids:
                self.logger.warning("æä¾›çš„ person_ids åˆ—è¡¨ç‚ºç©ºæˆ–ç„¡æ•ˆ")
                return pd.DataFrame()
            
            placeholders = ','.join(['%s'] * len(valid_ids))
            base_query += f" AND EXISTS (SELECT 1 FROM person_reference pr_filter WHERE pr_filter.reference_id = r.id AND pr_filter.person_id IN ({placeholders}))"
            params.extend(valid_ids)
            
        elif reference_ids:
            # ä½¿ç”¨æŒ‡å®šçš„ reference_id åˆ—è¡¨
            if not isinstance(reference_ids, (list, tuple)):
                reference_ids = [reference_ids]
            
            # éæ¿¾ç„¡æ•ˆçš„ ID
            valid_ids = [rid for rid in reference_ids if rid is not None and str(rid).strip()]
            
            if not valid_ids:
                self.logger.warning("æä¾›çš„ reference_ids åˆ—è¡¨ç‚ºç©ºæˆ–ç„¡æ•ˆ")
                return pd.DataFrame()
            
            placeholders = ','.join(['%s'] * len(valid_ids))
            base_query += f" AND r.id IN ({placeholders})"
            params.extend(valid_ids)
            
        elif min_reference_id is not None:
            # ä½¿ç”¨æœ€å° reference_id æ¢ä»¶
            # self.logger.info(f"å¾ reference_id >= {min_reference_id} ç²å–å°æ‡‰çš„æ–‡ç»")
            base_query += " AND r.id >= %s"
            params.append(min_reference_id)
            
        elif last_updated:
            # ä½¿ç”¨æœ€å¾Œæ›´æ–°æ™‚é–“
            base_query += f""" AND (
                -- æ¢ä»¶1: referencesçš„updated_at
                r.updated_at > %s
                OR 
                -- æ¢ä»¶2: personsçš„updated_at é€éperson_referenceè¡¨é—œè¯
                EXISTS (
                    SELECT 1 
                    FROM persons p2
                    INNER JOIN person_reference pr2 ON p2.id = pr2.person_id
                    WHERE pr2.reference_id = r.id 
                        AND p2.updated_at > %s
                )
            )
            """
            params.extend([last_updated, last_updated])
            
        elif update_all: 
            # æ›´æ–°å…¨éƒ¨ï¼Œä¸æ·»åŠ é¡å¤–æ¢ä»¶
            pass

        if limit:
            base_query += f" LIMIT {limit}"
        
        # åŸ·è¡ŒæŸ¥è©¢
        if params:
            results = self.execute_with_retry(base_query, tuple(params), fetch=True)
        else:
            results = self.execute_with_retry(base_query, fetch=True)

        if not results:
            return pd.DataFrame()
        
        # # åœ¨é€™è£¡è¨˜éŒ„å¯¦éš›æŸ¥è©¢åˆ°çš„çµæœæ•¸é‡
        # if person_ids:
        #     self.logger.info(f"å¾ person_ids æŸ¥è©¢åˆ° {len(results)} ç­†æ–‡ç»è¨˜éŒ„")
        # elif reference_ids:
        #     self.logger.info(f"å¾ reference_ids æŸ¥è©¢åˆ° {len(results)} ç­†æ–‡ç»è¨˜éŒ„")
        # elif min_reference_id is not None:
        #     self.logger.info(f"å¾æœ€å° reference_id æ¢ä»¶æŸ¥è©¢åˆ° {len(results)} ç­†æ–‡ç»è¨˜éŒ„")
        
        columns = ['last_name', 'first_name', 'middle_name', 'reference_id', 
                  'order', 'year', 'type', 'title', 'doi', 'article_title', 
                  'book_title', 'volume', 'issue', 'article_number', 'pages_range',
                  'edition', 'chapter', 'publish_date']
        
        df = pd.DataFrame(results, columns=columns).drop_duplicates().replace({np.nan: None})
        
        # æ¸…ç†è³‡æ–™ä¸­çš„ç‰¹æ®Šå­—ç¬¦
        if not df.empty:
            text_columns = ['article_title', 'book_title']
            for col in text_columns:
                if col in df.columns:
                    df[col] = df[col].apply(lambda x: str(x).replace('\\{\\}', '').strip() if x else None)
        
        return df
    
    def build_author_strings(self, authors_data):
        """æ§‹å»ºä½œè€…å­—ä¸²"""
        author_list = []
        short_author_list = []
        
        for _, row in authors_data.sort_values('order').iterrows():
            if row['last_name']:  # ç¢ºä¿æœ‰å§“æ°
                last_name = row['last_name']
                first_name = to_firstname_abbr(row['first_name'])
                middle_name = to_middlename_abbr(row['middle_name'])
                full_name = f"{last_name}, {first_name}{middle_name}"
                author_list.append(full_name)
                short_author_list.append(last_name)
        
        # æ ¼å¼åŒ–ä½œè€…å­—ä¸²
        if not author_list:
            return '', ''
        
        # å®Œæ•´ä½œè€…æ ¼å¼
        authors = {
            1: author_list[0],
            2: ' & '.join(author_list)
        }.get(len(author_list), ', '.join(author_list[:-1]) + ' & ' + author_list[-1])
        
        # ç°¡çŸ­ä½œè€…æ ¼å¼  
        short_authors = {
            1: short_author_list[0],
            2: ' & '.join(short_author_list)
        }.get(len(short_author_list), short_author_list[0] + ' et al.')
        
        return authors, short_authors
    
    def generate_content(self, row):
        """æ ¹æ“šæ–‡ç»é¡å‹ç”Ÿæˆå…§å®¹ï¼ˆä½¿ç”¨å·²æå–çš„æ¬„ä½ï¼‰"""
        content = row['title'] or ''
        
        content_generators = {
            3: self._generate_book_content,        # æ›¸ç±
            4: self._generate_catalog_content,     # åéŒ„  
            1: self._generate_journal_content,     # æœŸåˆŠæ–‡ç« 
            2: self._generate_chapter_content      # æ›¸ç±ç¯‡ç« 
        }
        
        generator = content_generators.get(row['type'])
        if generator:
            return generator(content, row)
        
        return content
    
    def _generate_book_content(self, content, row):
        """ç”Ÿæˆæ›¸ç±å…§å®¹"""
        return content + '.' if content and not content.endswith('.') else content
    
    def _generate_catalog_content(self, content, row):
        """ç”ŸæˆåéŒ„å…§å®¹"""
        return content  # ä¿æŒåŸæ¨£
    
    def _generate_journal_content(self, content, row):
        """ç”ŸæˆæœŸåˆŠæ–‡ç« å…§å®¹ï¼ˆä½¿ç”¨å·²æå–çš„æ¬„ä½ï¼‰"""
        # æ§‹å»ºå…§å®¹éƒ¨åˆ†
        content_parts = []
        
        # æ–‡ç« æ¨™é¡Œ
        article_title = row['article_title']
        if article_title:
            content_parts.append(f"{article_title}.")
        
        # æœŸåˆŠåç¨±å’Œå·è™Ÿéƒ¨åˆ†
        journal_part = ""
        book_title = row['book_title']
        if book_title:
            journal_part += f"<i>{book_title}</i>"
        
        volume = str(row['volume']).strip() if row['volume'] else ""
        if volume:
            if journal_part:  # å¦‚æœå‰é¢æœ‰æœŸåˆŠåç¨±ï¼ŒåŠ ç©ºæ ¼
                journal_part += f" {volume}"
            else:
                journal_part += volume
        
        # æœŸè™Ÿ - ç›´æ¥é™„åŠ åˆ°å·è™Ÿå¾Œé¢ï¼Œä¸åŠ ç©ºæ ¼
        issue = str(row['issue']).strip() if row['issue'] else ""
        if issue:
            journal_part += f"({issue})"
        
        # é ç¢¼æˆ–æ–‡ç« ç·¨è™Ÿ - ç›´æ¥é™„åŠ å†’è™Ÿï¼Œä¸åŠ ç©ºæ ¼
        if row['article_number']:
            journal_part += f": {row['article_number']}."
        elif row['pages_range']:
            journal_part += f": {row['pages_range']}."
        
        # å¦‚æœæœ‰æœŸåˆŠéƒ¨åˆ†ï¼ŒåŠ å…¥å…§å®¹
        if journal_part:
            content_parts.append(journal_part)
        
        # æœ€çµ‚çµ„åˆ
        return ' '.join(content_parts) if content_parts else content
    
    def _generate_chapter_content(self, content, row):
        """ç”Ÿæˆæ›¸ç±ç¯‡ç« å…§å®¹ï¼ˆä½¿ç”¨å·²æå–çš„æ¬„ä½ï¼‰"""
        article_title = row['article_title']
        book_title = row['book_title']

        content = f"{article_title}. In: {book_title}," if article_title and book_title else content
        
        # ç‰ˆæœ¬ã€å·è™Ÿã€ç« ç¯€è³‡è¨Š
        extras = []
        if row['edition']:
            extras.append(f"{row['edition']} ed.")
        if row['volume']:
            extras.append(f"vol. {row['volume']}")
        elif row['chapter']:
            extras.append(f"ch. {row['chapter']}")
            
        if extras:
            content += ' ' + ', '.join(extras) + '.'
            
        if row['pages_range']:
            content += f" {row['pages_range']}."
            
        return content

    def process_citations(self, results, get_publish_date=True):
        """è™•ç†å¼•ç”¨è³‡æ–™"""
        citation_data = []
        failed_references = []
        
        # æº–å‚™DOIå°æ‡‰çš„ç™¼å¸ƒæ—¥æœŸè³‡æ–™
        refs_dates = {}
        refs = results[['reference_id', 'doi', 'publish_date']].drop_duplicates()
        refs = refs.replace({'': None, nan: None})
        
        # è™•ç† DOI ç™¼å¸ƒæ—¥æœŸ
        if get_publish_date:
            # å…ˆå°‡æ‰€æœ‰å·²æœ‰çš„ç™¼å¸ƒæ—¥æœŸåŠ å…¥ refs_dates
            for _, row in refs.iterrows():
                refs_dates[row['reference_id']] = row['publish_date']
            
            # ç¯©é¸å‡ºéœ€è¦å¾ DOI ç²å–ç™¼å¸ƒæ—¥æœŸçš„è¨˜éŒ„
            refs_need_doi = refs[(refs['publish_date'].isna()) & (refs['doi'].notna())]
            
            if not refs_need_doi.empty:
                # self.logger.info(f"éœ€è¦å¾ DOI ç²å–ç™¼å¸ƒæ—¥æœŸçš„æ–‡ç»: {len(refs_need_doi)} ç­†")
                
                for _, row in refs_need_doi.iterrows():
                    try:
                        reference_id = row['reference_id']
                        publish_date = self.fetch_doi_publish_date(row['doi'])
                        refs_dates[reference_id] = publish_date
                        
                    except Exception as e:
                        self.logger.error(f"âŒ Reference ID {row.get('reference_id', 'Unknown')}: DOI è™•ç†å¤±æ•— - {e}")
                        refs_dates[row.get('reference_id')] = None
            # else:
            #     self.logger.info("æ‰€æœ‰æ–‡ç»éƒ½å·²æœ‰ç™¼å¸ƒæ—¥æœŸï¼Œç„¡éœ€å¾ DOI ç²å–")
        
        # è™•ç†æ¯å€‹ reference_id çš„å¼•ç”¨è³‡æ–™
        for ref_id in results['reference_id'].unique():
            try:
                ref_rows = results[results['reference_id'] == ref_id]
                
                # æ§‹å»ºä½œè€…å­—ä¸²
                authors, short_authors = self.build_author_strings(ref_rows)
                
                # ç²å–å¹´ä»½
                year = ref_rows['year'].iloc[0] if len(ref_rows) > 0 else ''
                
                # ç”Ÿæˆå…§å®¹
                content = self.generate_content(ref_rows.iloc[0])
                
                # ç²å–ç™¼å¸ƒæ—¥æœŸ
                if get_publish_date:
                    publish_date = refs_dates.get(ref_id)
                else:
                    publish_date = ref_rows['publish_date'].iloc[0]

                citation_data.append({
                    'reference_id': ref_id,
                    'author': f'{authors} ({year})' if authors else f'({year})',
                    'short_author': f'{short_authors}, {year}' if short_authors else str(year),
                    'content': content,
                    'publish_date': publish_date
                })
                
            except Exception as e:
                self.logger.error(f"âŒ Reference ID {ref_id}: è™•ç†å¤±æ•— - {e}")
                failed_references.append(ref_id)
                
                # å˜—è©¦å»ºç«‹æœ€åŸºæœ¬çš„è¨˜éŒ„ä½œç‚ºå¾Œå‚™
                try:
                    ref_rows = results[results['reference_id'] == ref_id]
                    basic_record = {
                        'reference_id': ref_id,
                        'author': f"({ref_rows['year'].iloc[0] if len(ref_rows) > 0 else ''})",
                        'short_author': str(ref_rows['year'].iloc[0] if len(ref_rows) > 0 else ''),
                        'content': ref_rows['title'].iloc[0] if len(ref_rows) > 0 else '',
                        'publish_date': None
                    }
                    citation_data.append(basic_record)
                    # self.logger.info(f"âœ… Reference ID {ref_id}: å·²å»ºç«‹åŸºæœ¬è¨˜éŒ„ä½œç‚ºå¾Œå‚™")
                except Exception as fallback_error:
                    self.logger.error(f"âŒ Reference ID {ref_id}: é€£åŸºæœ¬è¨˜éŒ„éƒ½ç„¡æ³•å»ºç«‹ - {fallback_error}")
        
        # å ±å‘Šè™•ç†çµæœ
        if failed_references:
            self.logger.warning(f"âš ï¸  å¤±æ•—çš„ Reference IDs ({len(failed_references)}): {failed_references}")
        
        return pd.DataFrame(citation_data)
    
    def update_citations_db(self, citation_df, update_time):
        """
        æ›´æ–°å¼•ç”¨è³‡æ–™å’Œç™¼å¸ƒæ—¥æœŸåˆ°æ•¸æ“šåº«ï¼Œä½¿ç”¨å„ªåŒ–çš„æ‰¹æ¬¡æ’å…¥
        
        Args:
            citation_df (pd.DataFrame): å¼•ç”¨è³‡æ–™DataFrame
            update_time: æ›´æ–°æ™‚é–“æˆ³
        """
        if citation_df.empty:
        #     self.logger.info("æ²’æœ‰è³‡æ–™éœ€è¦æ›´æ–°")
            return
            
        # ç¢ºä¿æ¬„ä½é †åºä¸¦æ·»åŠ æ™‚é–“æˆ³
        citation_df = citation_df[['reference_id', 'author', 'short_author', 'content', 'publish_date']].copy()
        citation_df['updated_at'] = update_time
        citation_df['created_at'] = update_time
        # citation_df.to_csv('citation_df.csv',index=None)
        
        # ä½¿ç”¨ INSERT ... ON DUPLICATE KEY UPDATE èªæ³•
        query = """
        INSERT INTO api_citations (reference_id, author, short_author, content, publish_date, updated_at, created_at) 
        VALUES (%s, %s, %s, %s, %s, %s, %s) 
        ON DUPLICATE KEY UPDATE 
            updated_at = CASE 
                WHEN COALESCE(author, '') != COALESCE(VALUES(author), '')
                     OR COALESCE(short_author, '') != COALESCE(VALUES(short_author), '')
                     OR COALESCE(content, '') != COALESCE(VALUES(content), '')
                     OR COALESCE(publish_date, '') != COALESCE(VALUES(publish_date), '')
                THEN VALUES(updated_at)
                ELSE updated_at
            END,
            author = VALUES(author), 
            short_author = VALUES(short_author), 
            content = VALUES(content), 
            publish_date = VALUES(publish_date)
        """
        
        # è½‰æ›ç‚ºå…ƒçµ„åˆ—è¡¨
        all_data = [tuple(row) for row in citation_df.values]
        
        # self.logger.info(f"æº–å‚™æ‰¹æ¬¡æ›´æ–° {len(all_data)} ç­† api_citations è³‡æ–™")
        
        # ä½¿ç”¨ DatabaseManager çš„æ‰¹æ¬¡åŸ·è¡Œæ–¹æ³•
        self.batch_execute(query, all_data, self.batch_size)
    
    def fetch_doi_publish_date(self, doi):
        """å¾DOIç²å–ç™¼å¸ƒæ—¥æœŸ"""
        if not doi:
            return None
        
        doi_clean = doi.replace('"', '').replace("'", '')
        if not doi_clean:
            return None
        
        try:
            url = f'https://api.crossref.org/works/{doi_clean}'
            response = requests.get(url, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                message = data.get('message', {})
                
                # å˜—è©¦ç²å–å°åˆ·ç™¼å¸ƒæ—¥æœŸ
                for date_type in ['published-print', 'published-online']:
                    try:
                        date_parts = message.get(date_type, {}).get('date-parts', [[]])[0]
                        if len(date_parts) >= 3:
                            return datetime(date_parts[0], date_parts[1], date_parts[2]).strftime("%Y-%m-%d")
                        elif len(date_parts) >= 2:
                            return datetime(date_parts[0], date_parts[1], 1).strftime("%Y-%m-%d")
                    except (IndexError, ValueError, TypeError):
                        continue
        except requests.RequestException:
            pass
        
        return None
    
    def set_batch_size(self, batch_size):
        """
        è¨­å®šæ‰¹æ¬¡å¤§å°
        
        Args:
            batch_size (int): æ–°çš„æ‰¹æ¬¡å¤§å°
        """
        if batch_size <= 0:
            raise ValueError("æ‰¹æ¬¡å¤§å°å¿…é ˆå¤§æ–¼ 0")
        
        self.batch_size = batch_size
        # self.logger.info(f"æ‰¹æ¬¡å¤§å°å·²è¨­å®šç‚º: {self.batch_size}")
    
    def run_update(self, custom_updated=None, limit=None, get_publish_date=True, update_all=False, reference_ids=None, person_ids=None, min_reference_id=None):
        """
        åŸ·è¡Œå®Œæ•´æ›´æ–°æµç¨‹
        
        Args:
            custom_updated: è‡ªå®šç¾©çš„æ›´æ–°æ™‚é–“
            limit: é™åˆ¶è™•ç†çš„è¨˜éŒ„æ•¸ï¼ˆç”¨æ–¼æ¸¬è©¦ï¼‰
            get_publish_date: æ˜¯å¦ç²å–ç™¼å¸ƒæ—¥æœŸ
            update_all: æ˜¯å¦æ›´æ–°å…¨éƒ¨
            reference_ids: æŒ‡å®šçš„ reference_id åˆ—è¡¨
            person_ids: æŒ‡å®šçš„ person_id åˆ—è¡¨
            min_reference_id: æœ€å° reference_id æ¢ä»¶
        """
        # self.logger.info("é–‹å§‹æ›´æ–°å¼•ç”¨è³‡æ–™...")
        # self.logger.info(f"ç•¶å‰æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        
        # 1. ç²å–æ›´æ–°æ™‚é–“
        last_updated_api_citation, last_updated_references = self.get_last_update_times()
        
        # # æ ¹æ“šä¸åŒçš„æ›´æ–°æ–¹å¼é¡¯ç¤ºä¸åŒçš„æ—¥èªŒ
        # if person_ids:
        #     if isinstance(person_ids, (list, tuple)):
        #         self.logger.info(f"æŒ‡å®šæ›´æ–° person_ids: {len(person_ids)} å€‹ ({person_ids[:10]}{'...' if len(person_ids) > 10 else ''})")
        #     else:
        #         self.logger.info(f"æŒ‡å®šæ›´æ–° person_id: {person_ids}")
        # elif reference_ids:
        #     if isinstance(reference_ids, (list, tuple)):
        #         self.logger.info(f"æŒ‡å®šæ›´æ–° reference_ids: {len(reference_ids)} å€‹ ({reference_ids[:10]}{'...' if len(reference_ids) > 10 else ''})")
        #     else:
        #         self.logger.info(f"æŒ‡å®šæ›´æ–° reference_id: {reference_ids}")
        # elif min_reference_id is not None:
        #     self.logger.info(f"æ›´æ–° reference_id >= {min_reference_id} çš„æ–‡ç»")
        # elif custom_updated:
        #     self.logger.info(f"è‡ªå®šç¾©æ›´æ–°æ™‚é–“: {custom_updated}")
        # elif update_all:
        #     self.logger.info("æ›´æ–°å…¨éƒ¨æ–‡ç»")
        # else:
        #     self.logger.info(f"å¼•ç”¨ä¸Šæ¬¡æ›´æ–°æ™‚é–“: {last_updated_api_citation}")
        
        # 2. ç²å–éœ€è¦æ›´æ–°çš„è³‡æ–™
        results = self.fetch_updated_references(
            last_updated=custom_updated if not person_ids and not reference_ids and not update_all and min_reference_id is None else None,
            limit=limit,
            update_all=update_all and not person_ids and not reference_ids and min_reference_id is None,
            reference_ids=reference_ids if not person_ids and min_reference_id is None else None,
            person_ids=person_ids if min_reference_id is None else None,
            min_reference_id=min_reference_id
        )

        if results.empty:
            # self.logger.info("æ²’æœ‰éœ€è¦æ›´æ–°çš„è³‡æ–™")
            return
        
        # self.logger.info(f"æ‰¾åˆ° {len(results['reference_id'].unique())} å€‹éœ€è¦æ›´æ–°çš„æ–‡ç»")
        
        # 3. è™•ç†å¼•ç”¨è³‡æ–™ï¼ˆåŒ…å«ç™¼å¸ƒæ—¥æœŸï¼‰
        citation_df = self.process_citations(results, get_publish_date=get_publish_date)
        
        # 4. æ›´æ–°å¼•ç”¨è³‡æ–™å’Œç™¼å¸ƒæ—¥æœŸ
        if not citation_df.empty:
            # self.logger.info("æ›´æ–°å¼•ç”¨è³‡æ–™å’Œç™¼å¸ƒæ—¥æœŸ...")
            self.update_citations_db(citation_df, last_updated_references)
        # else:
        #     self.logger.info("æ²’æœ‰è³‡æ–™éœ€è¦æ›´æ–°")
        
        # self.logger.info("æ›´æ–°å®Œæˆ!")


# ä½¿ç”¨ç¯„ä¾‹ï¼š

# def main():
#     """ä¸»å‡½æ•¸"""
#     # ä¸€èˆ¬åŸ·è¡Œ
#     with CitationUpdater(batch_size=1000, max_retries=3) as updater:
#         updater.run_update()
    
#     # æ¸¬è©¦
#     # with CitationUpdater(batch_size=1000, max_retries=3) as updater:
#     #     updater.run_update(get_publish_date=False)  # ä¸ç²å–ç™¼å¸ƒæ—¥æœŸï¼Œæé«˜é€Ÿåº¦
    
#     # é™åˆ¶æ•¸é‡æ¸¬è©¦
#     # with CitationUpdater(batch_size=1000, max_retries=3) as updater:
#     #     updater.run_update(limit=100)
    
#     # è‡ªå®šç¾©æ›´æ–°æ™‚é–“
#     # with CitationUpdater(batch_size=1000, max_retries=3) as updater:
#     #     updater.run_update(custom_updated=datetime(2024, 1, 1))
    
#     # ä½¿ç”¨ reference_id åˆ—è¡¨æ›´æ–°
#     # with CitationUpdater(batch_size=1000, max_retries=3) as updater:
#     #     updater.run_update(reference_ids=[1, 2, 3, 4, 5])
    
#     # ä½¿ç”¨å–®ä¸€ reference_id æ›´æ–°
#     # with CitationUpdater(batch_size=1000, max_retries=3) as updater:
#     #     updater.run_update(reference_ids=123)
    
#     # ä½¿ç”¨ person_id åˆ—è¡¨æ›´æ–°ï¼ˆæ›´æ–°é€™äº›äººå“¡ç›¸é—œçš„æ‰€æœ‰æ–‡ç»ï¼‰
#     # with CitationUpdater(batch_size=1000, max_retries=3) as updater:
#     #     updater.run_update(person_ids=[10, 20, 30, 40, 50])
    
#     # ä½¿ç”¨å–®ä¸€ person_id æ›´æ–°
#     # with CitationUpdater(batch_size=1000, max_retries=3) as updater:
#     #     updater.run_update(person_ids=456)
    
#     # ğŸ†• æ ¹æ“šæœ€å° reference_id æ›´æ–°
#     # with CitationUpdater(batch_size=1000, max_retries=3) as updater:
#     #     updater.run_update(min_reference_id=1000)

# if __name__ == "__main__":
#     main()



# ä½¿ç”¨æ–¹å¼ï¼š
# from scripts.final._01_update_ref import *
# 
# # ä½¿ç”¨ reference_id åˆ—è¡¨æ›´æ–°
# with CitationUpdater(batch_size=1000, max_retries=3) as updater:
#     updater.run_update(reference_ids=[1, 2, 3, 4, 5])
#
# # ä½¿ç”¨å–®ä¸€ reference_id æ›´æ–°  
# with CitationUpdater(batch_size=1000, max_retries=3) as updater:
#     updater.run_update(reference_ids=123)
#
# # ä½¿ç”¨ person_id åˆ—è¡¨æ›´æ–°ï¼ˆæ›´æ–°é€™äº›äººå“¡ç›¸é—œçš„æ‰€æœ‰æ–‡ç»ï¼‰
# with CitationUpdater(batch_size=1000, max_retries=3) as updater:
#     updater.run_update(person_ids=[10, 20, 30, 40, 50])
#
# # ä½¿ç”¨å–®ä¸€ person_id æ›´æ–°
# with CitationUpdater(batch_size=1000, max_retries=3) as updater:
#     updater.run_update(person_ids=456)
#
# # ğŸ†• æ ¹æ“šæœ€å° reference_id æ›´æ–°
# with CitationUpdater(batch_size=1000, max_retries=3) as updater:
#     updater.run_update(min_reference_id=1000)
#
# # å¯ä»¥çµåˆ limit åƒæ•¸æ§åˆ¶æ•¸é‡
# with CitationUpdater(batch_size=1000, max_retries=3) as updater:
#     updater.run_update(min_reference_id=1000, limit=5000)